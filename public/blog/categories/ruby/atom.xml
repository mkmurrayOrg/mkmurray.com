<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ruby | Mike Murray]]></title>
  <link href="http://mkmurray.com/blog/categories/ruby/atom.xml" rel="self"/>
  <link href="http://mkmurray.com/"/>
  <updated>2012-12-08T21:27:17-07:00</updated>
  <id>http://mkmurray.com/</id>
  <author>
    <name><![CDATA[Mike Murray]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Testing Trade-offs]]></title>
    <link href="http://mkmurray.com/blog/2012/12/08/testing-trade-offs/"/>
    <updated>2012-12-08T21:11:00-07:00</updated>
    <id>http://mkmurray.com/blog/2012/12/08/testing-trade-offs</id>
    <content type="html"><![CDATA[<p>Last week our dev team at Extend Health watched a <a href="http://www.youtube.com/watch?v=yTkzNHF6rMs">RubyConf 2012 video of a talk
entitled Boundaries</a> by <a href="https://twitter.com/garybernhardt">Gary
Bernhardt</a> of <a href="https://www.destroyallsoftware.com/screencasts">Destroy All
Software</a> (or perhaps even
better known for the infamous <a href="https://www.destroyallsoftware.com/talks/wat">WAT lightning talk at CodeMash
2012</a>). Gary proposes a very
interesting code architecture that marries the individual benefits of
immutability &amp; mutability, functional programming &amp; imperative object-oriented
programming, isolated unit testing &amp; integration testing. He discusses the pros
&amp; cons of each code design &amp; testing decision and the trade-offs that we end up
dealing with. He suggests a potential solution that makes virtually no trade-off
and attempts to harness the advantages of each of these methodologies that
appear to be at odds with one another. I feel the idea has a lot of merit and I
highly encourage everyone to watch his presentation to get the full context and
a better logical progression of his proposal than what I can provide.</p>

<h2>Isolated Unit Testing vs. Integration Testing</h2>

<p>A common practice in unit testing classes and objects is to isolate the targeted
class from its dependencies so that you can focus solely on its responsibilities
and domain logic independent of any implementation details of the dependencies.
This is typically accomplished by the use of stubs and mocks, which attempt to
control and monitor the interactions with and data return from dependencies.
There are a ton of gains afforded by this style of testing as Gary points out,
but one very large criticism of this testing methodology is that it doesn't
exercise the code in the same way it would run in production. It is not exactly
rare to have these isolated unit tests successfully pass and yet still have
problems in production.</p>

<p>Integration testing tends to better emulate production because it maintains the
interaction relationships between objects in addition to acquiring and passing
the data around the system in the same way. The criticism Gary puts forth of an
integrated testing strategy is that it is very slow as you begin to attempt to
cover all the code paths through the system. Consider trying to cover all
logical branches through the system, including all branches of
try/catch/finally, conditional, and looping structures. Gary suggests the growth
in code to cover these scenarios is 2<sup>n<sup>,</sup></sup> where <code>n</code> is the number of branches.</p>

<p>Many try to get the benefits of both types of testing in order to compensate for
each strategy's shortcomings. However, they still write code that doesn't play
to each testing methodolgy's strengths. The strength of isolated unit testing is
verifying that given certain inputs the expected output is always returned. The
strength of integration testing is coordinating dependencies, making sure they
utilize and interact with the API of other objects correctly. If you design your
classes to be a mix of dependencies and logic, it becomes difficult to
effectively test cover them without writing both sets of tests for all classes
in your codebase.</p>

<h2>So What?</h2>

<p>OK, so you are probably wondering what the point is then. This sounds like a
good plan to just be more disciplined in your test coverage, right? Well, let's
explore a way to better segregate dependency orchestration from actual logic and
behavior. This could allow us to use the right testing methodology depending on
which type of responsibility the object is meant to encapsulate: dependencies or
decisions? Gary also asserts that it will lead us down a path that could yield a
codebase with better modularity, scalability, and potentially even concurrency.
I believe you can also add better maintainability and extensibility to that
list.</p>

<p>Next week I will dive into how Gary suggests we can better seperate these
concerns of dependencies and decisions. Stay tuned (or go watch the video and
spoil the surprise).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Refactoring Legacy Code Starts with Test Coverage]]></title>
    <link href="http://mkmurray.com/blog/2012/11/10/refactoring-legacy-code-starts-with-test-coverage/"/>
    <updated>2012-11-10T15:37:00-07:00</updated>
    <id>http://mkmurray.com/blog/2012/11/10/refactoring-legacy-code-starts-with-test-coverage</id>
    <content type="html"><![CDATA[<p>As a dev team, we have been watching and discussing various training videos and
conference talks every morning for a few months now. This week we watched a
recorded conference talk by Ruby developer <a href="http://kytrinyx.com/">Katrina Owen</a>
entitled <a href="http://www.youtube.com/watch?v=J4dlF0kcThQ">Therapeutic Refactoring</a>.
I was especially impressed with her strategy for refactoring legacy code.</p>

<p>I'm sure everyone can agree that the safest way to refactor code and be 100%
positive that the functionality hasn't regressed is to have a good suite of
tests. And if you don't have sufficient test coverage (or any at all), then now
is the right time to add it before refactoring.</p>

<p>However, I have found myself skipping this step in the past, feeling justified
because the quality of the code was so poor or the deadline was so short. It is
irresponsible of me to assume that I have not changed the functionality of the
code without proof in the form of passing tests that sufficiently cover all
scenarios and use cases.  Nonetheless, watching Katrina's presentation has
given me renewed commitment and a sound plan of attack against legacy code that
may be ugly and very unfamiliar to me.</p>

<p>Here is the example method that Katrina uses during her presentation (which is
later revealed as a piece of code that she vaguely remembers coding herself and
now abhors):</p>

<pre class="brush: ruby">
    module XYZService

      def self.xyz_filename(target)
        # File format:
        # [day of month zero-padded][three-letter prefix] \
        # _[kind]_[age_if_kind_personal]_[target.id] \
        # _[8 random chars]_[10 first chars of title].jpg
        filename = "#{target.publish_on.strftime("%d")}"
        filename << "#{target.xyz_category_prefix}"
        filename << "#{target.kind.gsub("_", "")}"
        filename << "_%03d" % (target.age || 0) if target.personal?
        filename << "_#{target.id.to_s}"
        filename << "_#{Digest::SHA1.hexdigest(rand(10000).to_s)[0,8]}"
        truncated_title = target.title.gsub(/[^\[a-z\]]/i, '').downcase
        truncate_to = truncated_title.length > 9 ? 9 : truncated_title.length
        filename << "_#{truncated_title[0..(truncate_to)]}"
        filename << ".jpg"
        return filename
      end

    end
</pre>


<p>It certainly isn't the worst lines of code ever written and it is getting the
job done, but it is definitely not very readable and quite difficult to
understand its purpose and logic. About the only thing that can be discerned is
that it is intended to generate a filename in a specific format.</p>

<p>Katrina starts by writing a very simple and seemingly useless test (which she
calls a "Mickey Mouse test" in her GitHub commit message) in order to begin test
coverage of this method:</p>

<pre class="brush: ruby">
    require_relative './xyz_service'

    describe XYZService do

      let(:target) do
        stub(:target)
      end

      subject { XYZService.xyz_filename(target) }

      it 'works' do
        subject.should eq('something')
      end

    end
</pre>


<p>Without much knowledge of what this method's purpose or inputs are, we can only
pass in an empty stub object as input and then assert something rediculous for
the result. We expect this test to not only fail to pass, but it is also likely
to error during runtime (or even at compile time if in a statically typed
language like C#). The idea is to create a quick feedback loop that aids us in
finding the context and inputs required to make the code execute without runtime
error. Then we can finally see the actual return value and modify our dummy
assertion to begin expecting that output.</p>

<p>After iterating in this fashion, Katrina arrives at the following test which is
a bit more useful:</p>

<pre class="brush: ruby">
    require_relative './xyz_service'
    require 'date'

    describe XYZService do

      let(:target) do
        messages = {
          :publish_on => Date.new(2012, 3, 14),
          :xyz_category_prefix => 'abc',
          :kind => 'unicorn',
          :personal? => false,
          :id => 1337,
          :title => 'magic & superglue'
        }
        stub(:target, messages)
      end

      subject { XYZService.xyz_filename(target) }

      it 'works' do
        subject.should eq('14abcunicorn_1337_cb6c53bc_magicsuper.jpg')
      end

    end
</pre>


<p>However, this test doesn't pass because the <code>cb6c53bc</code> portion of the resultant
filename is different every execution of the test. A simple regex allows the
test to pass all of the time:</p>

<pre class="brush: ruby">
    subject.should match(/14abcunicorn_1337_[0-9a-f]{8}_magicsuper\.jpg/)
</pre>


<p>Even though we now have a passing test, we have not yet exercised the different
input values and all code paths and branches through the method. This must be
done in order to fully understand, document, and test cover the logic and
purpose of the method. This would be done in the normal unit testing fashion
that is shown in just about every testing demo by adding more test scenarios and
assertions that excercise the varying use cases.</p>

<p>One interesting development that occurred while Katrina was using this process
was that she actually found a bug in the implementation of the original
<code>xyz_filename</code> method, specifically an extra set of braces in the regex on line
14.</p>

<p>Now that we are fully covered with tests (and a bug fixed), we are free to
refactor the code into smaller, more readable chunks of code logic that are more
cohesive and more reusable as well. This is what Katrina does for the remainder
of her presentation, and it is just as instructive as this first part. However,
today I wanted to focus on her strategy for writing test coverage against a
legacy codebase.</p>

<p>I like how what Katrina is demonstrating seems to utilize a black box testing
approach at first, and then progresses to a more white box approach as we begin
to explore the different parts of the implementation. Perhaps such a strategy
should have been obvious to me, but Katrina's conference talk really struck a
chord with me and encouraged me to commit to doing better with test coverage in
legacy codebases before I begin making modifications or additions.</p>

<p>I highly recommend viewing the entire video, as it is only 30 minutes long and
an entertaining watch.</p>
]]></content>
  </entry>
  
</feed>
